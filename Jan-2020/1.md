Deep Q-learning from Demonstrations:
The agent can access data from previous control of the system, the ratio of demonstration data is automaticaly assessed using
prioritized replay mechanism.

### Keypoints: 

It is pretrains solely on the demonstration data using TD learning and supervised loss. After that, it starts interacting with
the domain. The agent updates its network with a mix of demonstration and self-generated data. Choosing the ratio between
the demonstration and self-generated data while learning is critical. 

(Piot, Geist, and Pietquin 2014b) showed that adding TD loss to the supervised classification loss improves imitation learning 
even when there are no rewards.

Human Experience Replay use a replay buffer that is mixed between agent and demonstration data but gains were only slightly better
than a random agent. 

The agent is pre-trained for good performance (using demonstration data only), and the expert data is kept permanently in the
buffer.

The goal of the pre-training phase is learn to imitate the demonstrator with a value function that satisfies the Bellman error
equation so that it can be updated with TD updates once the agent starts interacting with the environment. During this phase, 
the agent samples mini-batches from the demontration data and updates the network using four losses: 1-step double Q-learning
loss, an n-step double Q-learning loss, a supervised large margin classification loss, and an L2 regularization loss on the 
weights and biases. 

Once the pre-training phase is completed, the agent start interacting with the environment and collect self-gerating data, adding
it to the replay buffer. The demonstration data is kept permanently. For proportional prioritized sampling, different small
positive constants are added to control the relative sampling of demonstration data.

They found that in many games where human players are better than DQN, it was due to DQN being trained with all rewards clipped
to 1. For the most difficult games, the demonstration data is sampled more frequently over time. For most other games, the ratio converges to a near constant level, which differs for each game. 

Naively adding (e.g. only pre-training or filling the replay buffer) a small amount of demonstration does not provide similar
benefit and can sometimes be detrimental. 

### Thoughts:

