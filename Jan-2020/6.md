Multi-Modal Imitation Learning in Partially Observable Environments
### General:
Learn policies from expert demonstration without reward signal. Consider multi-expert learning with 2-D environment with expert
trajectories consists of two human-distinguishable behaviors. Also test with continuous control locomotion tasks. Combine the 
advantage of InfoGAIL and RNN-nased belief state representation. 

### Key points
- Existing RL methods (BC, IRL, GAIL) are restricted to learning from single-expert demonstrations for a single task under full
observability. In reality, expert demonstrations are from several experts with different levels of expertises, expertise in 
different tasks, or different presences even at the same situation. They extend InfoGAIL to pomdp settings with RNN-based belief
state representation. They joinly learn the belief state rep with the policy and critic modules. 
- A general technique in pomdb relies on RNN which embeds the history trajectory (observations + actions) as a hidden state b_{t-1}. Combining the current observation z_t, action a_t and previous state embedding b_{t-1}, the RNN network produces a new belief b_t. Unlike InfoGAIL, they aim to maximize the mutual information between latent code and belief representations of generated trajectories. Unlike BMIL, they don't require belief regularization to avoid mode collapses, since the belieft state representation for pi, D and P are three independent RNN models.
- Experiments: PPO + GAE to train the agents' policies. Use GRU, 128 hidden cells and bootstrapped random update. 
- Environments: Multi-expert 2D trajectories: expert demonstrations consits of two sets of optimal behaviors. The observation is a noisy 2D coordinate of the current position of the agent, the action is the moving direction with a unit-length movement. The log standard deviation of noise in moving direction is 1. Multi-task Mujoco: forward and backward Hopper-v3. Reducing the sensor to make it become pomdp. 
