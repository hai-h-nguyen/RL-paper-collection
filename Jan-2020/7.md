Learning Belief Representations for Imitation Learning in POMDPs

### General
Learning from expert demonstration in pomdp. Belief representation is learned jointly with the policy using a task-aware imitation loss. To improve the robustness, several regularization techniques are used. Compared with GAIL on partially observable continuous control locomotion tasks. Have an ablation study to identify the effectiveness of task-aware belief learning and belieft regularization. 

### Key Points:
- Algorithms to perform approximate inference of the belief state representation from raw observation using RNN, variational autoencoder and predicitve state representatiosn. After the belief model has been learned, a policy optimization algorithm is then applied to the belief representation to optimize a predefined reward signal. 
- GAIL: imitation learning from MDPs, little focus on pomdp. 
- Losses to regularize: predicting multi-step past/future observations and action-sequences. 
