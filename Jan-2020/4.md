Learning Latent Plans from Play (place-holder)

## General
This is not a RL paper. They proposed a self-supervising control from human teleoperated play data to scale up skill learning. 
Compared to convetional task demonstration, play data is cheap and can be collected in large quantities without task segmenting, labeling, or resetting to an initial state. Play is naturally rich, convering ~4x more interaction space than demonstrations for the same amount of collection time (in their experiments). Introduce Play-MLP, a method that learns to organize play behaviors in a latent space then reuse them at test time to achieve specific goals. The combine self-supervised control with a play dataset and the proposed approach outperform individual expert-trained policies on 18 tasks in a simulated robotic tabletop environment. Additionaly, the model are more robust to pertubations and exhibit retrying-till-success behaviors.

## Key points
- Motivated by the idea of a robot generalist: a single agent can learn a wide variety of skills. This remains a challenge in robotics
- Perform RL in this scenario is hard because for each skill, it is required to have manually designed individual reward for each task. 
- Motivated by the idea of an agent capable of __task-agnostic control__: the ability to reach any reachable goal state form any current state. In this setting, the notion of task is no longer discrete, but continuous - indexed by the pair (current state s_c - goal state s_g). Cite Unsupervied control through non-parametric discriminative rewards. Learning this can be formalized as the search for a goal-conditioned policy \pi(a|s_c, s_g)
- The data to train is pairs of (s_c, s_g) and actions that connect current and goal states. The problem is it the difficulty to have this kind of dataset. Random exploration is typically insufficiently rich to cover enough the state space for complex manipulation task. Expert demonstrations can be arbitrarily complex but are expensive to collect and only visit a narrow training distribution over visited states, leading to distribution shift at test time. 
- The approach is to use unlabeled play data collected while a human teleoperate the robot and perform actions that satisfies their own curiosity. Play is not random but structured by human knowledge of object affordances, this makes play much more discriminate than random scripting. We can expect play to cover an environment's interaction space. 
- Normaly, goal conditioned policies are learned using RL. But they used supervised learning which introduces a major challenge, as the distribution over actions can reach a temporally distant goal from the current state based on the data can be highly mulimodal. That limits single-task imitation models to short and relatively simplt tasks such as pushing, re-positioning rope, or short-distance navigation. 
- Generating the data: A human given the current state s_c, formulate a mental image of a goal state s_g, driven by curiousity or some instrinsic motivation. Then consider a prior distribution over all valid ways of reach s_g from s_c p (b|s_c, s_g), a behavior repertoire encoding knowledge of object affordances and environment dynamics. 
- Play-supervised goal-conditioned behavioral cloning: D is a play dataset: paired (O_t, a_t). O_t is the set of observations from each of the robot's N sensory channels, a_t is the action. In their experiment, O = {I, p}: I is an image from a fixed first-person viewpoint, p is the 8DOF state. 
- The key idea is a random window of (observation, action) pairs extracted from play describes exactly how the robot go from a particular initial state to a particular goal. It is guaranteed that the final state is reachable from the initial state under the intevening actions. This is exploited to create a self-supervised labeling scheme: the initial state of a random sequence as current state, the final state as reachable goal state, and the actions is the label.
