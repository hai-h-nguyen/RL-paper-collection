Deep Reinforcement Learning with Double Q-learning
### General: 
- Q-learning is known to overestimate action values under certain conditions. This idea was introduced in tabular
setting but now it is extended to DQN. The idea is to use two networks: one for choosing the action (through max operation) and 
one for evaluate actions. The idea is to decomposing the max operation in the target into action selection and action evaluation
(not fully decoupled).

### Key points
- Certain situations: past: insufficiently flexible function approximation and noise. The paper showed overestimation also occurs when
the action values are inaccurate, irrespective of the source of approximation error.
- Optimisim in the face of uncertainty can be a good exploration technique. If however, the overestimations are not uniform and
not concentrated at states that we want to learn more, they might have bad affect. 
- Q-learning's overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain
random erros uniformly distributed in an interval [-epsilon, epsilon], then the target is overestimated up to gamma x epsilon x 
(m-1)/(m+1) where m is the number of actions. Later, van hasselt (2010) argued that the noise in the environment can lead to 
overestimations even when using tabular representation and proposed Double Q-learning.
- The over-estimations increase with the number of actions. Overestimations can become worse with boostrapping. 
- Using the main network to evaluate the action and using the target network to estimate actions.
- The network has 1.5M params, trained on a single CPU for 200M frames (1 week clock time). The true values of (state-action)
pairs are the actual discounted value of the best learned policy averaged over 125,000 steps. They run 6 different random seeds
and they average the two extreme values to obtain the shaded area. 
- Experiments showed overestimation affect negatively the learned policies. However, it does not always adversely affect the
quality of the learned policy. For example, DQN achieves optimal behavior in Pong despite slightly overestimatin the policy 
value. 
- Evaluation test are run on epsilon-greedy with epsilon=0.05. They also use small epsilon=0.001 during evaluation. Learning rate 0.00025
. The update steps between target updates is 10,000. The agent is evaluated every 1M steps and the best policy across the 
evaluations is kept as the output of the learing process. training freq = 4, batchsize = 32, exploration policy reducing from 1
to 0.1 over 1M steps. RMS prop with momentum 0.95. 

### Thoughts
- The standard training procedure for DQN-variant should be: train with epsilon-greedy with annealed epsilon from 1.0 to some 
small threshold (0.1). Then the best policy is saved at the end. Evaluation the policy with some small epsilon. 
