Dueling Network Architectures for Deep Reinforcement Learning

### General
The proposed network represents two separate estimators: one for the state value function and one for advantage function. The 
main benefit is to generalize learning across actions without imposing any change to the underlying rl algorithm. The results
show it leads to better policy evaluation in the presence of many similar-valued actions. The two streams are combined via 
a sepcial agregating layer to produce Q.

### Key points:
- The intuition is that the dueling architecture can learn which states are (or are not) valuable, without having to learn the
effect of each aciton for each state. This is helpful in states where actions do not affect the environment in any relevant way.
- Use saliency map (computing the jacobians of the trained value and andvantage streams with respect to the input video) - from
Simonyan et al (2013). 
- This together with prioritized replay produce sota results (better than double dqn). 
- Advantage updating was shown to converge faster than Q-learning in simple continuous time domains. Advantage function obtains
a relative measure of the importance of each action. 
- The key insights is that for many states, it is unnecessary to estimate the value of each action choice. For games such as 
Enduro, knowing whether to move left or right only matters when a collision is eminent. In some states, it is of paramount 
important to know which action to take, but in many other states the choice is not that important. 
- Prioritized replay: Built on top of double DQN (DDQN). The idea is to increase the replay probability of experience tuples 
that have a high expected learning progress (measured by absoluted TD-error). 
- To produce Q from V and A: Q = V + (A - 1/|A| \sum_a' A(s,a';\theta, \alpha). This loses the original semantics of V and A
(off-target by a constant), but it increases the stability of the optimization: the advantage only need to change as fast 
as the mean. 
- They experiment by first evaluating a given policy (to isolate factors such as the choice of exploration strategy, and the 
interaction between policy improvement and policy evaluation). They compared the error between the estimated value function and
the true value function on a corridor environment. Results show the duelling structure converges faster when the number of actions increases.
- They clip the gradients at 10 (common trick in recurrent network training). Also, they rescale the combined gradient entering the last convolutional layer by 1/sqrt(2) since both the advantage and the value stream propoagate the gradients.
- Gradient clipping seems important.

### Thoughts:
