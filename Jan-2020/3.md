Dueling Network Architectures for Deep Reinforcement Learning

### General
The proposed network represents two separate estimators: one for the state value function and one for advantage function. The 
main benefit is to generalize learning across actions without imposing any change to the underlying rl algorithm. The results
show it leads to better policy evaluation in the presence of many similar-valued actions. The two streams are combined via 
a sepcial agregating layer to produce Q.

### Key points:
- The intuition is that the dueling architecture can learn which states are (or are not) valuable, without having to learn the
effect of each aciton for each state. This is helpful in states where actions do not affect the environment in any relevant way.
- Use saliency map (computing the jacobians of the trained value and andvantage streams with respect to the input video) - from
Simonyan et al (2013). 
- This together with prioritized replay produce sota results (better than double dqn). 
- Advantage updating was shown to converge faster than Q-learning in simple continuous time domains. Advantage function obtains
a relative measure of the importance of each action. 
- The key insights is that for many states, it is unnecessary to estimate the value of each action choice. For games such as 
Enduro, knowing whether to move left or right only matters when a collision is eminent. In some states, it is of paramount 
important to know which action to take, but in many other states the choice is not that important. 

### Thoughts:
