The paper is the first to tackle 3D environments (VizDoom Game) that involves POMDP. During training, they exploit feature
information such as the presence of enemies or items (in binary values). The model is trained to learn these features
and minimizing the Q-learning Bellman error. Their architecture is also modularized with different models to be independently
trained for different phases of one hard mode of the game (deathmatch scenarios).
Source code: https://github.com/glample/Arnold

When using LSTM, we estimate Q(h_t, a_t). Their approach is built on top of the DRQN architecture. They tested the barebone DRQN
and it performed well with simple scenarios (available actions were to turn or attack). The agent performed worse on deathmatch
mode. 

Co-training with feature detection: They added two FC layesr of size 512 and k to the output of the CNN with k is the game features
that they want to detect. At training time, the cost inclues the normal DRQN cost and the cross-entropy loss. LSTM only takes
input the CNN output, not the game features. The output game features are only used for the additional loss calculation. 

They also test with different structure: to use a separate network to make predictions and reinjecting the predicted features
into the LSTM. The results was bad. Sharing the CNN is critical. 

Divide and conquer: For the deathmatch task they have one normal DQN network for navigation and one DRQN+augmented features
for action. During evaluation, the action network is called at each step. If no enemies is detected or there are no ammo left, 
the navigation network is called. 

Reward shaping: To tackle the sparsity of the reward they use shaped rewards such as positive reward for object pickup, negative
reward for loosing health, shooting or loosing ammo. For the navigation network: positive reward when it picks up an item and
negative reward for walking on lava. Also positive reward for the distance travelled.

Sequential update: A sequence of n observations o_1, o_2, ..., o_n is randomly sampled from the replay memory but instead of
updating all action-state in the sequence, they only considers the ones that are provided with enough history. Because the 
first states of the sequence will be estimated from an almost non-exisitent history (since h_0 is reinitialized at the beginning
of the updates) and might be inaccurate. Therefore, errors from states o_1, o_2, ..., o_h (h is the minimum history size for 
a state to be updated), are not backpropagated. Errors from other states will be backpropagated, o_n being used to create a
target for o_{n-1} action-state. In there experiment, they set the minimum history size to 4 and they perform the updates on 
5 states. Increasing the number of updates leads to high correlation in sampled frames, violating the DQN sampling policy. 
Decreasing the nuber of updates makes it very difficult to converge to a good policy.

Experiments: Drop-out is important for the action network on the limited death-match.
